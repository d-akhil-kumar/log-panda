# Kubernetes & Minikube ‚Äì Detailed Notes

## üåê What is Kubernetes?
- **Definition**: Kubernetes (K8s) is a container orchestration platform that automates deployment, scaling, and management of containerized applications.
- **Why it‚Äôs used**:
  - Automates container scheduling across nodes.
  - Handles scaling (up/down) automatically.
  - Provides self-healing (restarts failed pods).
  - Manages networking (service discovery, DNS).
  - Supports rolling updates and rollbacks.

---

## üèó Kubernetes Architecture
- **Control Plane Components**:
  - **kube-apiserver** ‚Üí entrypoint for all commands (`kubectl` talks to this).
  - **etcd** ‚Üí distributed key-value store (cluster state).
  - **kube-scheduler** ‚Üí assigns pods to nodes based on resources.
  - **kube-controller-manager** ‚Üí ensures desired state (replicas, nodes).
- **Node Components**:
  - **kubelet** ‚Üí agent on each node, manages pods/containers.
  - **kube-proxy** ‚Üí handles networking/routing rules for pods.
  - **Container runtime** ‚Üí Docker/Containerd/CRI-O, actually runs containers.

---

## üñ• Practicing Locally with Minikube
- **Minikube** simulates a Kubernetes cluster locally (1-node).
- **Installation (Linux)**:
  ```bash
  curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
  sudo install minikube-linux-amd64 /usr/local/bin/minikube
  ```
- **Start cluster**:
  ```bash
  minikube start --driver=docker
  ```
- **Check cluster**:
  ```bash
  kubectl get nodes
  kubectl get pods -A
  ```

- **Clean up the existing Minikube cluster**:
  ```bash
  minikube stop
  minikube delete
  minikube start --driver=docker
  ```


- **To enable metrics server for HPA and VPA resource calculations**:
  ```bash
  minikube addons enable metrics-server
  ```



---

## üì¶ Pods
- Smallest deployable unit in Kubernetes.
- Encapsulates **one or more containers** + shared resources (network, storage).
- Commands:
  ```bash
  kubectl get pods
  kubectl logs <pod-name>
  kubectl exec -it <pod-name> -- /bin/sh
  kubectl delete pod <pod-name>
  ```

---

## üìë Deployments
- Manages replicas of Pods (via ReplicaSets).
- Supports scaling, rolling updates, rollbacks.
- Key elements:
  - **apiVersion**: apps/v1
  - **kind**: Deployment
  - **metadata**: name, labels
  - **spec.replicas**: number of pods
  - **spec.selector**: which pods this Deployment manages
  - **spec.template**: defines pod spec (containers, env, ports)
- Scaling:
  ```bash
  kubectl scale deployment log-panda-app1 --replicas=5
  ```
- Deleting:
  ```bash
  kubectl delete deployment log-panda-app1
  ```

---

## üè∑ Pods vs Deployments vs ReplicaSets vs StatefulSets
- **Pod** ‚Üí single instance of your container(s).
- **ReplicaSet** ‚Üí ensures a specific number of pods are running (not used directly, managed by Deployment).
- **Deployment** ‚Üí manages ReplicaSets, enables scaling + rolling updates. Good for stateless apps (like app1/app2/app3).
- **StatefulSet** ‚Üí manages stateful apps (DBs, Kafka, etc.), gives stable identity + persistent storage.

---

## üîå Services
- Abstraction to expose Pods internally/externally.
- Types:
  - **ClusterIP (default)** ‚Üí internal-only.
  - **NodePort** ‚Üí exposes on each node‚Äôs IP:port (range 30000‚Äì32767). Used in dev, rarely in prod.
  - **LoadBalancer** ‚Üí provisions external LB (on cloud providers).
- Industry practice:
  - Internal services ‚Üí ClusterIP
  - External access ‚Üí Ingress + LoadBalancer

---

## üåê Ingress
- Manages external access via **hostnames/paths**.
- Requires an **Ingress Controller** (NGINX, Traefik, etc.).
- In Minikube:
  ```bash
  minikube addons enable ingress
  ```
- Use `/etc/hosts` to map `app1.local` ‚Üí Minikube IP.

---

## üê≥ Docker Images & Kubernetes
- Pods pull images from registries (Docker Hub, GCP, ECR, etc.).
- Workflow:
  1. Build image locally ‚Üí `docker build -t username/app1:v1.0.0 .`
  2. Test locally ‚Üí `docker run -p 3000:3000 username/app1:v1.0.0`
  3. Push to Docker Hub ‚Üí  
     ```bash
     docker login
     docker push username/app1:v1.0.0
     ```
  4. Reference in Deployment YAML under `image:`.

---

## üìã Environment Variables in Pods
- Set directly in Deployment YAML under `spec.containers.env`.
- Precedence: **Deployment env vars > Image baked `.env` in src dir**.
- Example:
  ```yaml
  env:
    - name: PORT
      value: "3000"
  ```

---

## ‚è≥ Jobs
- A Job ensures that **a pod runs to completion** (used for batch tasks like data processing, migrations, one-time jobs).
- Unlike Deployments (which keep pods running), Jobs **terminate once done**.
- Can be parallelized with `.spec.parallelism` and `.spec.completions`.

---

## üõ† Useful Commands Recap
```bash
kubectl get pods                # list pods
kubectl get svc                 # list services
kubectl logs -f <pod>              # view pod logs [if there are multiple containers running within a pod, then this will fetch the logs of first container by default]
kubectl exec -it <pod> -- sh    # shell inside container
kubectl apply -f file.yaml      # apply manifest
kubectl delete -f file.yaml     # delete resource
minikube ip                     # get Minikube IP
minikube ssh                    # login to node
minikube service <svc-name>     # open service in browser
```

---

## Kafka in Kubernetes (Production Considerations)
- Kafka is **stateful** and tightly coupled with storage & networking performance.
- Running Kafka on Kubernetes is possible but adds complexity.
- Typical issues:
  - Scaling **up** requires **manual partition reassignment** (Kafka does not automatically rebalance partitions).
  - Scaling **down** is risky (must safely move partitions off brokers).
  - StatefulSets + PVCs help, but **dynamic scaling like stateless apps is not feasible**.

### Options for Production Kafka
1. **Managed Kafka Services (Preferred)**  
   - AWS MSK, Confluent Cloud, Aiven, Redpanda Cloud.  
   - Pros: No operational burden, automatic scaling, monitoring, and upgrades.

2. **Self-Hosted Kafka (on VMs or Bare Metal)**  
   - Run on dedicated servers with tuned SSD/NVMe disks.
   - Greater control over performance, but more maintenance overhead.

3. **Kafka in Kubernetes (with Operators)**  
   - Tools like **Strimzi Operator** or **Confluent Operator** automate broker lifecycle, upgrades, and partition reassignment.  
   - Suitable if an organization standardizes on Kubernetes for all infra.  
   - Still requires careful planning for scaling.

## Key Takeaways
- **Don‚Äôt treat Kafka like a Deployment** ‚Üí It cannot auto-scale seamlessly.
- **KRaft removes ZooKeeper** but does **not** eliminate scaling complexity.
- For learning and small setups ‚Üí running Kafka in Kubernetes is fine.  
- For production ‚Üí **prefer managed Kafka** or a well-maintained operator.



## Headless Services in Kubernetes
- A Service with `clusterIP: None` is called a **Headless Service**.
- Instead of a single ClusterIP for load-balancing, it returns the **DNS records of individual Pods** behind it.
- Useful for StatefulSets like Kafka, where clients need to talk to specific brokers by stable DNS names (e.g., `kafka-0.service-name:9092`).


## üåÄ Headless Services (`clusterIP: None`)

Normally, when you create a **Service** in Kubernetes:

- It gets a **ClusterIP** (a single **virtual IP / VIP**).  
- Clients connect to the Service IP, and kube-proxy does load balancing across the Pods that match the Service selector.  
- Example:
  ```bash
  kubectl get svc log-panda-app1-service
  ```
  Output:
  ```
  NAME                      TYPE        CLUSTER-IP    PORT(S)    AGE
  log-panda-app1-service    ClusterIP   10.96.120.47  3000/TCP   5m
  ```
  Here, `10.96.120.47` is the **VIP**. DNS (`log-panda-app1-service.default.svc.cluster.local`) resolves to that single VIP.

---

### üîé What happens with `clusterIP: None`?
If you set:
```yaml
spec:
  clusterIP: None
```

- Kubernetes does **not assign a ClusterIP**.  
- Instead, the Service name directly resolves to the **individual Pod IPs** that match the selector.  
- Example with a StatefulSet (`kafka` with 3 replicas):

```bash
kubectl get pods -l app=kafka -o wide
```
Output:
```
NAME       READY   STATUS    IP           NODE
kafka-0    1/1     Running   10.244.0.12  worker-1
kafka-1    1/1     Running   10.244.1.15  worker-2
kafka-2    1/1     Running   10.244.2.20  worker-3
```

Now do a DNS lookup:
```bash
kubectl run -it --rm dnsutils --image=infoblox/dnstools -- nslookup kafka
```

Output:
```
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:   kafka.default.svc.cluster.local
Address: 10.244.0.12
Address: 10.244.1.15
Address: 10.244.2.20
```

üëâ Instead of one VIP, you get **all Pod IPs**.

---

### ‚ö° Why is this useful?
- For **StatefulSets (Kafka, ZooKeeper, DBs, etc.)** where each Pod has a stable identity.  
- Clients often need to talk to a **specific broker** (not just load-balanced).  
- Kubernetes gives each StatefulSet Pod a stable DNS name:
  ```
  kafka-0.kafka.default.svc.cluster.local
  kafka-1.kafka.default.svc.cluster.local
  kafka-2.kafka.default.svc.cluster.local
  ```
- This way, producers/consumers can directly connect to `kafka-0`, `kafka-1`, etc., instead of being routed randomly.

---

‚úÖ In short:  
- **Normal Service (ClusterIP)** ‚Üí One VIP, kube-proxy load balances.  
- **Headless Service (`clusterIP: None`)** ‚Üí No VIP, DNS returns **Pod IPs**. Perfect for StatefulSets.  


# Kubernetes HPA (Horizontal Pod Autoscaler) ‚Äì Notes

## üîπ What is HPA?
- HPA automatically scales the number of **pods** in a Deployment, ReplicaSet, or StatefulSet based on observed metrics.
- Metrics can be **CPU**, **memory**, or **custom metrics** (e.g., requests per second).

---

## üîπ How HPA Works
- Kubernetes continuously monitors the metrics of pods.
- Compares **current average** with **target value**.
- Adjusts replicas according to a formula.

---

## üîπ Formula for Desired Replicas
```
desiredReplicas = currentReplicas * (currentAverage / targetAverage)
```

- **currentReplicas** ‚Üí number of pods currently running.
- **currentAverage** ‚Üí average metric (CPU, memory, etc.) across all pods.
- **targetAverage** ‚Üí metric threshold you set in HPA.

---

## üîπ Scale-Up Example

**Scenario**:  
- Target CPU: 50%  
- Current Pods: 3  
- CPU usage per pod: `[60%, 70%, 65%]`  

**Step 1: Calculate average**  
```
Average CPU = (60 + 70 + 65) / 3 = 65%
```

**Step 2: Apply formula**  
```
desiredReplicas = 3 * (65 / 50) = 3 * 1.3 = 3.9 ‚âà 4 pods
```

**Result:** HPA will scale **up from 3 ‚Üí 4 pods**.

---

## üîπ Scale-Down Example

**Scenario**:  
- Target CPU: 50%  
- Current Pods: 4  
- CPU usage per pod: `[40%, 45%, 35%, 50%]`  

**Step 1: Calculate average**  
```
Average CPU = (40 + 45 + 35 + 50) / 4 = 42.5%
```

**Step 2: Apply formula**  
```
desiredReplicas = 4 * (42.5 / 50) = 4 * 0.85 = 3.4 ‚âà 3 pods
```

**Result:** HPA will scale **down from 4 ‚Üí 3 pods** after cooldown.

---

## üîπ Important Notes on Scaling
1. **Scale-up**: Happens immediately when metrics exceed target.  
2. **Scale-down**: Cooldown period (default 5 min) prevents rapid scaling down.  
3. **Min/Max replicas**: HPA respects `spec.minReplicas` and `spec.maxReplicas`.  
4. **Average metric**: HPA calculates the average across all pods, not individual spikes.  
5. **Multiple metrics**: If using multiple metrics, HPA uses the **highest required scale** among them.

---

## üîπ Metrics Triggers
| Metric Type         | Trigger Example                                 |
|--------------------|-----------------------------------------------|
| CPU                 | Average CPU > 70% ‚Üí scale up                  |
| Memory              | Average memory > 80% ‚Üí scale up               |
| Custom metrics      | Requests/sec, queue length, etc.              |
| External metrics    | External systems like Prometheus, Kafka lag   |

---

## üîπ Adding HPA to Deployment YAML

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: log-panda-log-ingest-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: log-panda-log-ingest
  minReplicas: 1
  maxReplicas: 5
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50
```

- HPA monitors **Deployment `log-panda-log-ingest`**.  
- Scales between **1‚Äì5 pods** based on **50% CPU utilization**.

---

## ‚úÖ Summary
- HPA is a reactive autoscaler based on metrics.  
- Scale-up is immediate, scale-down respects cooldown.  
- Always set **min/max replicas** to avoid extreme scaling.  
- Metrics can be **CPU, memory, custom, or external**.
- if in hpa min replica is 1 and if you scale down the pod to 0, then hpa will not spawn a new pod. 



# Vertical Pod Autoscaler (VPA) Notes

## üåü What is VPA?
- **Definition**: VPA automatically adjusts the CPU and memory requests/limits for pods based on historical and current usage.
- **Goal**: Ensure pods have the right resources without manual tuning.

---

## üîß Components
- **VPA object**: Attached to a Deployment/StatefulSet.
- **Recommender**: Monitors usage and provides recommendations.
- **Updater**: Evicts pods to apply new resource requests.
- **Admission Controller**: Ensures new pods start with recommended resources.

---

## üìù Example VPA Manifest
```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: log-panda-log-ingest-vpa
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: log-panda-log-ingest
  updatePolicy:
    updateMode: "Auto"
```

### Explanation
- `targetRef`: Points to the Deployment whose pods you want to autoscale vertically.
- `updateMode: Auto`: VPA will automatically evict pods and restart them with updated resources.

---

## ‚ö° How it Works
1. VPA observes pod metrics over time.
2. Recommender suggests CPU/memory requests.
3. If `updateMode: Auto`:
   - Pods are evicted.
   - New pods are scheduled with updated CPU/memory.
4. If `updateMode: Off`:
   - Only recommendations are provided; no changes are applied.

### Important Notes
- **Update causes pod eviction**: Pod restarts may impact availability; combine with HPA for best results.
- **Works best for stateful apps**: Databases, Kafka brokers, etc., where horizontal scaling might be limited.
- **Cannot prevent pod eviction if mode is `Auto`**.

---

## ‚úÖ Best Practices
- Use HPA for horizontal scaling and VPA for vertical scaling together.
- Start with `updateMode: "Off"` in production to monitor recommendations.
- Test evictions carefully to avoid downtime.
- Monitor VPA logs:
  ```bash
  kubectl describe vpa <vpa-name>
  ```

---

## Kubernetes Probes: Liveness, Readiness, Startup

Kubernetes uses **probes** to determine the health and lifecycle state of a container.  
They ensure that Pods are restarted or traffic is directed correctly, depending on the container‚Äôs behavior.

---

#### 1. **Liveness Probe**
- **Purpose**: Checks if the container is **still alive**.
- If it fails ‚Üí Kubernetes **kills the container** and restarts it.
- Used for: detecting **deadlocks, infinite loops, or crashes** where the container is running but not working.

#### Example:
```yaml
livenessProbe:
  httpGet:
    path: /healthz
    port: 8080
  initialDelaySeconds: 10
  periodSeconds: 5
  failureThreshold: 3
````

---

####  2. **Readiness Probe**

* **Purpose**: Checks if the container is **ready to serve traffic**.
* If it fails ‚Üí Kubernetes **removes the Pod from Service endpoints** (no traffic is routed).
* Container stays alive, but no traffic until it passes again.
* Used for: apps that take time to warm up or load data before serving.

#### Example:

```yaml
readinessProbe:
  tcpSocket:
    port: 5432
  initialDelaySeconds: 15
  periodSeconds: 10
```

---

####  3. **Startup Probe**

* **Purpose**: For containers that take **long time to initialize**.

* During startup probe phase:

  * Liveness & Readiness probes are **disabled**.
  * Only once startup succeeds ‚Üí Liveness & Readiness begin.

* If startup fails ‚Üí Pod is killed and restarted.

* Used for: Kafka, Postgres, Elasticsearch, or apps with **heavy bootstrapping**.

### Example:

```yaml
startupProbe:
  tcpSocket:
    port: 9092
  failureThreshold: 30   # allows 30 * 10s = 5 minutes startup time
  periodSeconds: 10
```

---

####  ‚ö° How They Work Together

1. **Startup ‚Üí Readiness ‚Üí Liveness**

   * At Pod creation:

     * `startupProbe` runs (if defined).
     * Until it passes, `readinessProbe` & `livenessProbe` are **ignored**.
   * After startup passes:

     * `readinessProbe` decides if Pod can receive traffic.
     * `livenessProbe` ensures Pod is still alive.

2. **If no startupProbe is defined**:

   * Liveness & Readiness begin **immediately** after container start.

---

####  üîë Best Practices

* **Always add `readinessProbe`** for apps exposed via Services ‚Üí prevents routing traffic too early.
* **Use `startupProbe`** for slow-starting apps (Kafka, Postgres, Elasticsearch, etc.).
* **LivenessProbe should be lightweight** (simple HTTP/TCP checks). Avoid expensive queries.
* **Don‚Äôt duplicate logic**:

  * If you have a startupProbe, keep readiness/liveness aggressive.
* **Failure thresholds matter**:

  * Too strict ‚Üí false restarts.
  * Too loose ‚Üí longer downtime before restart.

---

####  üöÄ Example: Kafka with Probes

```yaml
startupProbe:
  tcpSocket:
    port: 9092
  failureThreshold: 30
  periodSeconds: 10

readinessProbe:
  tcpSocket:
    port: 9092
  initialDelaySeconds: 30
  periodSeconds: 10

livenessProbe:
  tcpSocket:
    port: 9092
  initialDelaySeconds: 60
  periodSeconds: 20
```

* `startupProbe`: waits up to 5 min for Kafka to bootstrap.
* `readinessProbe`: marks broker ready only after it accepts TCP.
* `livenessProbe`: restarts broker if it later becomes unresponsive.

---

‚úÖ With this setup:

* Kafka won‚Äôt get killed while booting.
* It won‚Äôt receive traffic until it‚Äôs ready.
* If it freezes after running, K8s restarts it.

```

---

Would you like me to also append this probe explanation into your **main running notes file** (so it stays with HPA, VPA, StatefulSets, etc.) or keep it as a **separate probes.md**?
```


Here‚Äôs a **.md formatted note** on Kubernetes Probes, their use cases, and how they relate to each other:

````markdown
# Kubernetes Probes (Liveness, Readiness, Startup)

Kubernetes uses **probes** to monitor the health of containers and decide whether to restart them, send traffic to them, or wait for them to become ready.  
There are **three types of probes**: `livenessProbe`, `readinessProbe`, and `startupProbe`.

---

## 1. Liveness Probe ü©∫
- **Purpose**: Checks if the container is **still alive** (responsive).  
- **If it fails** ‚Üí Kubernetes **restarts the container**.  
- Example use case:
  - Web server stuck in a deadlock (process alive, but not responding).  
  - Kafka node hung due to GC or thread lock.  

```yaml
livenessProbe:
  httpGet:
    path: /healthz
    port: 8080
  initialDelaySeconds: 10
  periodSeconds: 15
  failureThreshold: 3
````

---

#### 2. Readiness Probe ‚úÖ

* **Purpose**: Checks if the container is **ready to serve requests/traffic**.
* **If it fails** ‚Üí Pod is marked **NOT READY**, removed from Service endpoints, but **not restarted**.
* Example use case:

  * Database container still initializing schemas.
  * Kafka broker started, but not yet connected to Zookeeper/Controller.

```yaml
readinessProbe:
  tcpSocket:
    port: 9092
  initialDelaySeconds: 30
  periodSeconds: 10
  failureThreshold: 6
```

---

####  3. Startup Probe üöÄ

* **Purpose**: Handles **slow-starting containers** (like Kafka, databases, or legacy apps).
* **Runs first**, and only after it succeeds do `livenessProbe` and `readinessProbe` start.
* **If it fails** ‚Üí Container is killed and restarted.
* Prevents premature failures caused by long boot times.

```yaml
startupProbe:
  tcpSocket:
    port: 9092
  failureThreshold: 30    # allows up to 30 * 10s = 5 minutes startup
  periodSeconds: 10
```

---

####  üîÑ How They Work Together

1. **Startup Probe ‚Üí Readiness + Liveness**

   * While `startupProbe` is running, **liveness** and **readiness** are ignored.
   * Once `startupProbe` succeeds, **liveness** and **readiness** start checking.

2. **Readiness ‚â† Liveness**

   * Readiness failing **does not restart** the Pod ‚Üí it just stops receiving traffic.
   * Liveness failing **restarts** the Pod.

3. **Common Flow**

   * Pod starts ‚Üí `startupProbe` checks.
   * If passes ‚Üí Pod enters liveness + readiness checks.
   * If readiness fails ‚Üí Pod not added to Service.
   * If liveness fails ‚Üí Pod restarted.

---

#### ‚öñÔ∏è When to Use Which?

* **Only livenessProbe** ‚Üí For apps that start fast and rarely hang.
* **livenessProbe + readinessProbe** ‚Üí For most production apps.
* **All three (startup + readiness + liveness)** ‚Üí For heavy/slow apps like Kafka, databases, Elasticsearch, etc.

---

####  Example Combined Setup (Kafka)

```yaml
startupProbe:
  tcpSocket:
    port: 9092
  failureThreshold: 30
  periodSeconds: 10

readinessProbe:
  tcpSocket:
    port: 9092
  initialDelaySeconds: 30
  periodSeconds: 10
  failureThreshold: 6

livenessProbe:
  tcpSocket:
    port: 9092
  initialDelaySeconds: 60
  periodSeconds: 15
  failureThreshold: 3
```

---

####  üéØ Key Tips

* **Use `startupProbe` for apps with long init times** (avoids false restarts).
* **Set `readinessProbe` for traffic safety** (don‚Äôt send traffic to unready pods).
* **Always define `livenessProbe` in prod** to auto-heal deadlocks/hangs.
* Tune `initialDelaySeconds`, `failureThreshold`, and `periodSeconds` carefully depending on app behavior.



Sure! Here‚Äôs a clean **.md** version for your notes on ConfigMaps and Secrets, covering theory, best practices, and useful commands:

---

# Kubernetes ConfigMaps & Secrets

## ConfigMap

* **Purpose:**
  Store **non-sensitive configuration data** separately from your container images. Examples: application settings, URLs, feature flags, config files.

* **Characteristics:**

  * Data is **plain text**.
  * Can be injected as **environment variables** or **mounted as files** in pods.
  * Useful for **decoupling config from code**.

* **When to use:**

  * Application configuration that **does not contain sensitive information**.
  * Config that **may change over time** without rebuilding the container image.

* **Example:**

  * Database host, feature toggle, log level, API endpoint.

---

## Secret

* **Purpose:**
  Store **sensitive information** such as passwords, tokens, API keys, and certificates.

* **Characteristics:**

  * Data is **base64 encoded**, not encrypted by default.
  * Can be injected as **environment variables** or **mounted as files**.
  * Access controlled using **RBAC**.
  * Stored in **etcd**, which can (and should) be encrypted at rest in production.

* **When to use:**

  * Credentials for databases or services.
  * TLS certificates, API keys, tokens.
  * Anything sensitive that **should not appear in plain text in YAML or images**.

* **Security Notes:**

  * **Base64 is not secure**: it‚Äôs only encoding. Anyone with access can decode it.
  * **RBAC**: Only users or service accounts with permissions can read the Secret.
  * **Etcd encryption**: Enable to protect Secrets at rest.
  * Secrets can be updated without redeploying the pod (if mounted as a volume).

---

## Best Practices

| Aspect                 | ConfigMap                    | Secret                                 |
| ---------------------- | ---------------------------- | -------------------------------------- |
| Sensitive Data         | ‚ùå                            | ‚úÖ                                      |
| Mount Options          | Env vars / Volume            | Env vars / Volume                      |
| Stored in etcd         | Yes, plain text              | Yes, base64 encoded (encrypt at rest)  |
| Update Without Restart | Yes, if mounted as volume    | Depends on application                 |
| Use Case Examples      | Feature flags, URLs, configs | DB credentials, API keys, certificates |

---

## Commands

### Create ConfigMap

```bash
kubectl create configmap <name> --from-literal=KEY=VALUE
kubectl create configmap <name> --from-file=path/to/file
```

### Create Secret

```bash
# Using literal values
kubectl create secret generic <name> \
  --from-literal=POSTGRES_USER=log_panda_db_user \
  --from-literal=POSTGRES_PASSWORD=ptm_pg_2024
```

* No need to manually create YAML files for Secrets if using `kubectl create secret`.

### View Secret YAML

```bash
kubectl get secret <name> -o yaml
```

* Values are **base64 encoded**.
* Decode using:

```bash
echo "<base64-value>" | base64 --decode
```

---

### RBAC & Access Control

* Secrets are **protected via Kubernetes RBAC**:

  * Only pods or users with appropriate **roles** can access Secrets.
  * For example, you can give a **service account** permission to read only specific Secrets.

* **Etcd encryption:**

  * In production, encrypt Secrets at rest to prevent access if someone gains direct access to the etcd datastore.

---

This should give you a **complete theoretical note** without implementation details, but includes commands for CLI usage.

---

I can also make a **downloadable `.md` file** for you containing this. Do you want me to do that?
