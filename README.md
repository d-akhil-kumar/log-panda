# ğŸ¼ Log Panda â€“ Event Driven Logging System  

ğŸ¼ Real-time log pipeline to explore **event-driven architecture** with **Kafka**, **Debezium**, **NestJS**, **Postgres**, **MySQL** and **Elasticsearch** â€” built to learn modern data streaming, observability, and scaling with Docker &amp; Kubernetes.

The system ingests logs from multiple apps, publishes them to Kafka, and processes them via consumers for storage and analysis.  

---

## ğŸš€ Project Flow  

![Flow](./log-panda.png)


1. **Apps (App1 / App2 / App3)**  
   - Dummy services that generate logs and send them via HTTP POST to the **Log Ingest** service.  

2. **Log Ingest (NestJS)**  
   - Acts as a gateway to receive logs.  
   - Validates payloads (DTOs with class-validator).  
   - Publishes messages to Kafka (`log-ingest-topic`).  

3. **Kafka (Broker + Init Service)**  
   - Runs in **KRaft mode** (no ZooKeeper).  
   - Topics are auto-created on container start (via init script).  
   - Provides durability and scaling with partitions.  

4. **Log DB Ingest (NestJS)**  
   - Kafka consumer service.  
   - Reads logs from Kafka in batches (configurable).  
   - Inserts processed logs into **Postgres**.  

---

## ğŸ—ï¸ Monorepo Structure  

```
log-panda/
â”‚â”€â”€ apps/
â”‚   â”œâ”€â”€ app1/                 # Dummy log producer app
â”‚   â”œâ”€â”€ app2/
â”‚   â”œâ”€â”€ app3/
â”‚   â”œâ”€â”€ log-ingest/           # NestJS service (producer)
â”‚   â””â”€â”€ log-db-ingest/        # NestJS service (consumer + Postgres)
â”‚
â”‚â”€â”€ docker/
â”‚   â”œâ”€â”€ kafka.Dockerfile      # Kafka image
â”‚   â””â”€â”€ scripts/
â”‚       â””â”€â”€ create-topics.sh  # Auto-create Kafka topics
â”‚
â”‚â”€â”€ docker-compose.yml        # Ingest + Kafka + DB consumer
â”‚â”€â”€ docker-compose-test-apps.yml  # Dummy apps (App1/App2/App3)
â”‚â”€â”€ README.md                 # You're reading it ğŸ™‚
```

---

## âš¡ Getting Started  

### 1. Clone the repo  
```bash
git clone https://github.com/your-org/log-panda.git
cd log-panda
```

### 2. Start postgres  
```bash
docker compose -f docker-compose-pg.yml up --build
```

This brings log-db-postgres which stores the logs data.


### 3. Start Kafka + Ingest + DB Consumer  
```bash
docker compose up --build
```

This brings up:  
- `log-ingest` â†’ NestJS service for ingesting logs.  
- `log-ingest-kafka` â†’ Kafka broker (KRaft).  
- `log-ingest-kafka-init` â†’ Init container to create topics.  
- `log-db-ingest` â†’ NestJS consumer service (logs â†’ Postgres).  


### 4. Start Dummy Apps  
```bash
docker compose -f docker-compose-test-apps.yml up --build
```

This brings up App1, App2, App3, each generating logs and sending them to `log-ingest`.

---

## ğŸ› ï¸ Useful Commands  

### Check Kafka topics
```bash
docker exec -it log_panda_log_ingest_kafka_c kafka-topics.sh   --bootstrap-server localhost:9092 --list
```

### Produce a message manually
```bash
docker exec -it log_panda_log_ingest_kafka_c kafka-console-producer.sh   --broker-list localhost:9092 --topic log-ingest-topic
```

### Consume messages manually
```bash
docker exec -it log_panda_log_ingest_kafka_c kafka-console-consumer.sh   --bootstrap-server localhost:9092 --topic log-ingest-topic --from-beginning
```

---

## ğŸ“¦ Data Flow Recap  

1. **App1/App2/App3 â†’ Log Ingest**  
   - `POST /ingest` â†’ JSON log payload  

2. **Log Ingest â†’ Kafka**  
   - Publishes messages with **idempotent producer** enabled  

3. **Kafka â†’ Log DB Ingest**  
   - Consumer group ensures parallelism across partitions  
   - Logs are batched and stored in Postgres  

---

## ğŸ”® Next Steps  

- Add **Kafka Connect** for syncing logs to **Elasticsearch (EKS)**.  
- Add **Prometheus + Grafana** for observability.  
- Scale with **Kubernetes** for multiple consumers.  
- Implement **idempotency at DB layer** for strong guarantees.  

---

âœ¨ With this setup, you already have a **real-world style event-driven pipeline** running locally.  